---
title: "MSDS 6371 Project - Fall 2022"
author: "Andrew Yule, Krithika Kondakindi"
format: pdf
editor: visual
editor_options: 
  chunk_output_type: console
---

# Introduction

# Data Description

# Analysis

### Question 1

Brief introduction to the questions of interest and the setting of the problem.

#### Restatement of the Problem

(Where did the data come from? How big is it? How many observations? Where can we find out more? What are the specific variables that we need to know with respect to your analysis?)

#### Build and Fit the Model

#### Checking the Assumptions

Residual Plots Influential point analysis (Cook's D and Leverage) Make sure to address each assumption.

#### Comparing Competing Models

Adj R2 Internal CV Press

#### Parameters

Estimates Interpretation Confidence Intervals

#### Conclusion

A short summary of the analysis.

### R Shiny: Price v. Living Area Chart

### Question 2

#### Restatement of Problem

#### Model Selection

Type of Selection Stepwise Forward Backward Custom

#### Checking Assumptions

Residual Plots Influential point analysis (Cook's D and Leverage) Make sure to address each assumption

#### Comparing Competing Models

Adj R2 Internal CV Press
Kaggle Score

#### Conclusion

A short summary of the analysis

# Appendix

Load required libraries

```{r}
library(tidyverse)
library(broom)
library(terra)
```

Read training and test data and perform any cleaning steps

```{r}
housesTrain = read_csv("train.csv")
housesTest = read_csv("test.csv")

# Remove ID column only from the training set
housesTrain = select(housesTrain, -Id)

# Find which columns have missing values between the training and test data sets
colsToDrop = housesTrain |>
  select_if(function(x) any(is.na(x))) |>
  colnames()
colsToDrop2 = housesTest |>
  select_if(function(x) any(is.na(x))) |>
  colnames()

# Remove any of the columns that had missing values
housesTrain =  housesTrain |>
  select(-starts_with(c(colsToDrop, colsToDrop2)))
housesTest =  housesTest |>
  select(-starts_with(c(colsToDrop, colsToDrop2)))

# From the plot below, LotArea should likely cut any value off at 30,000 sq ft
housesTrain$LotArea = clamp(housesTrain$LotArea, lower = 0, upper = 30000)
housesTest$LotArea = clamp(housesTest$LotArea, lower = 0, upper = 30000)

# From the plot below, GrLivArea should likely cut any value off at 4000 sq ft
housesTrain$GrLivArea = clamp(housesTrain$GrLivArea, lower = 0, upper = 4000)
housesTest$GrLivArea = clamp(housesTest$GrLivArea, lower = 0, upper = 4000)

# From the plot below, MSSubClass looks to have a quadratic relationship

# From the plot below, LotArea should should have a log relationship, which means each value should be greater than or equal to 1
housesTrain$LotArea = clamp(housesTrain$LotArea, lower = 1)
housesTest$LotArea = clamp(housesTest$LotArea, lower = 1)

# From the plot below, OpenPorchSF should should have a log relationship, which means each value should be greater than or equal to 1
#housesTrain$OpenPorchSF = clamp(housesTrain$OpenPorchSF, lower = 1)
#housesTest$OpenPorchSF = clamp(housesTest$OpenPorchSF, lower = 1)

# From the plot below, MiscVal should should have a log relationship, which means each value should be greater than or equal to 1
housesTrain$MiscVal = clamp(housesTrain$MiscVal, lower = 1)
housesTest$MiscVal = clamp(housesTest$MiscVal, lower = 1)

# From the plot below, PoolArea should should have a log relationship, which means each value should be greater than or equal to 1
#housesTrain$PoolArea = clamp(housesTrain$PoolArea, lower = 1)
#housesTest$PoolArea = clamp(housesTest$PoolArea, lower = 1)

# Screen for outliers by looking at the histograms of any variables that are all numeric
housesTrain |> 
  select(where(is.numeric)) |>
  gather() |>
  ggplot(aes(x = value)) + 
    facet_wrap(~key, scales = "free") + 
    geom_histogram()

# MoSold should be a factor instead of integer
housesTrain$MoSold = factor(housesTrain$MoSold, levels = seq(1, 12))
housesTest$MoSold = factor(housesTest$MoSold, levels = seq(1, 12))

# The sale price has a wide variance. Applying a log transformation helps
housesTrain |>
  ggplot(aes(x = log(SalePrice))) +
  geom_histogram()

# Log of SalePrice should be used to reduce the variance
housesTrain$SalePrice = log(housesTrain$SalePrice)

```

Full model creation

```{r}
housesTrain |> count(`3SsnPorch`)

fit = lm(SalePrice ~ I(MSSubClass^2) + log(LotArea) + Street + LotShape + LandContour + LotConfig + LandSlope + Neighborhood + Condition1 + Condition2 + BldgType + HouseStyle + OverallQual + OverallCond + YearBuilt + YearRemodAdd + RoofStyle + RoofMatl + ExterQual + ExterCond + Foundation + Heating + HeatingQC + CentralAir + `1stFlrSF` + `2ndFlrSF` + LowQualFinSF + GrLivArea + FullBath + HalfBath + BedroomAbvGr + KitchenAbvGr + TotRmsAbvGrd + Fireplaces + PavedDrive + WoodDeckSF + OpenPorchSF + EnclosedPorch + `3SsnPorch` + ScreenPorch + PoolArea + log(MiscVal) + MoSold + YrSold + SaleCondition, data = housesTrain)
summary(fit)

```

Test the basic model against the test data set and submit to Kaggle to see the score

```{r}
results = housesTest
results$SalePrice = exp(predict.lm(fit, newdata = housesTest))
results = results |>
  select(Id, SalePrice)

# Export
write_csv(results, "test_submission_10.csv")
```

Try using forward stepping approach

```{r}
# Define intercept-only model, this is also just the median sale price
intercept_only = lm(SalePrice ~ 1, data = housesTrain)

# Define model with all predictors
all = lm(SalePrice ~ ., data = housesTrain)

# Perform forward step-wise regression
forward = step(intercept_only, direction = 'forward', scope = formula(all), trace = 0)

# View final model
summary(forward)

```

Test the forward model against the test data set and submit to Kaggle

```{r}
results = housesTest
results$SalePrice = exp(predict.lm(forward, newdata = housesTest))
results = results |>
  select(Id, SalePrice)

# Export
write_csv(results, "test_submission_forward_1.csv")
```

Try using backward stepping approach

```{r}
# Define model with all predictors
all = lm(SalePrice ~ ., data = housesTrain)

# Perform forward step-wise regression
backward = step(all, direction = 'backward', scope = formula(all), trace = 0)

# View final model
summary(backward)

```

Test the backward model against the test data set and submit to Kaggle

```{r}
results = housesTest
results$SalePrice = exp(predict.lm(backward, newdata = housesTest))
results = results |>
  select(Id, SalePrice)

# Export
write_csv(results, "test_submission_backward_1.csv")
```

Try using stepwise stepping approach

```{r}
# Define intercept-only model, this is also just the average fat amount
intercept_only = lm(SalePrice ~ 1, data = housesTrain)

# Define model with all predictors
all = lm(SalePrice ~ ., data = housesTrain)

# Perform forward step-wise regression
both = step(intercept_only, direction = 'both', scope = formula(all), trace = 0)

# View final model
summary(both)
```

Test the stepwise model against the test data set and submit to Kaggle

```{r}
results = housesTest
results$SalePrice = exp(predict.lm(both, newdata = housesTest))
results = results |>
  select(Id, SalePrice)

# Export
write_csv(results, "test_submission_stepwise_1.csv")
```

